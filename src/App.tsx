import { useState, useRef, useEffect } from "react";
import * as tf from "@tensorflow/tfjs";
import * as mobilenet from "@tensorflow-models/mobilenet";
import Tesseract from "tesseract.js";
import Fuse from "fuse.js";
import "./App.css";

interface ScanResult {
    cardId: string;
    cardName: string;
    similarity: number; // cosine similarity in [0, 100]
}

interface Hit {
    cardId: string;
    similarity: number;
}

interface EmbeddingData {
    card_id: string;
    filename: string;
    embedding: number[];
}

interface EmbeddingsDatabase {
    model: string;
    embedding_dim: number;
    total_images: number;
    total_cards: number;
    embeddings: EmbeddingData[];
}

function App() {
    const [isScanning, setIsScanning] = useState(false);
    const [validHit, setValidHit] = useState<ScanResult | null>(null);
    const [hitHistory, setHitHistory] = useState<Hit[]>([]);
    const [error, setError] = useState<string | null>(null);
    const [modelStatus, setModelStatus] = useState<string>("Loading model...");
    const [isModelLoaded, setIsModelLoaded] = useState(false);

    const videoRef = useRef<HTMLVideoElement>(null);
    const canvasRef = useRef<HTMLCanvasElement>(null);
    const streamRef = useRef<MediaStream | null>(null);
    const intervalRef = useRef<number | null>(null);

    // MobileNet model reference
    const modelRef = useRef<mobilenet.MobileNet | null>(null);
    const embeddingsDatabaseRef = useRef<EmbeddingsDatabase | null>(null);
    const pokemonNamesFuseRef = useRef<Fuse<string> | null>(null);

    // ðŸ§  Load MobileNet feature extractor + embeddings database + Pokemon names
    useEffect(() => {
        async function loadModel() {
            try {
                setModelStatus("Loading embeddings database...");

                // Load embeddings database (generated by TypeScript script)
                const embeddingsResponse = await fetch("/embeddings/embeddings.json");
                const embeddingsData: EmbeddingsDatabase = await embeddingsResponse.json();
                embeddingsDatabaseRef.current = embeddingsData;

                // console.log(`âœ… Loaded ${embeddingsData.total_images} embeddings for ${embeddingsData.total_cards} cards`);
                // console.log(`   Model: ${embeddingsData.model}`);
                // console.log(`   Embedding dimension: ${embeddingsData.embedding_dim}`);

                setModelStatus("Loading Pokemon names database...");

                // Load Pokemon names for fuzzy matching
                const pokemonNamesResponse = await fetch("/pokemon-names.json");
                const pokemonNames: string[] = await pokemonNamesResponse.json();

                // Create Fuse instance for fuzzy searching
                pokemonNamesFuseRef.current = new Fuse(pokemonNames, {
                    threshold: 0.4, // 0 = perfect match, 1 = match anything
                    distance: 100,
                    includeScore: true,
                });

                console.log(`âœ… Loaded ${pokemonNames.length} Pokemon names for fuzzy matching`);

                setModelStatus("Loading MobileNet model...");

                // Load MobileNet v2 from TensorFlow.js (same as used for embeddings generation)
                const featureExtractor = await mobilenet.load({
                    version: 2,
                    alpha: 1.0, // Full model (not quantized)
                });
                modelRef.current = featureExtractor;

                // console.log("âœ… MobileNet v2 loaded successfully");

                setModelStatus(`Model loaded! ${embeddingsData.total_cards} cards ready (${embeddingsData.total_images} variations)`);
                setIsModelLoaded(true);
            } catch (err) {
                console.error("Failed to load model:", err);
                setError("Failed to load model: " + (err as Error).message);
                setModelStatus("Model load failed");
                setIsModelLoaded(false);
            }
        }

        loadModel();
    }, []);

    // ðŸ“¸ Start camera and scanning
    const startScanning = async () => {
        if (!modelRef.current) {
            setError("Model not loaded yet. Please wait...");
            return;
        }

        try {
            setError(null);
            setValidHit(null); // Clear previous valid hit
            setHitHistory([]); // Clear hit history

            // Request camera access
            const stream = await navigator.mediaDevices.getUserMedia({
                video: {
                    facingMode: "environment",
                    width: { ideal: 720 },
                    height: { ideal: 1000 },
                    aspectRatio: { ideal: 0.72 },
                },
            });

            if (videoRef.current) {
                videoRef.current.srcObject = stream;
                streamRef.current = stream;
                setIsScanning(true);

                // Start capturing frames every 500ms (slowed for OCR debugging)
                intervalRef.current = window.setInterval(() => {
                    captureAndPredict();
                }, 500);
            }
        } catch (err) {
            setError("Failed to access camera: " + (err as Error).message);
        }
    };

    // ðŸ›‘ Stop camera and scanning
    const stopScanning = () => {
        if (intervalRef.current) {
            clearInterval(intervalRef.current);
            intervalRef.current = null;
        }

        if (streamRef.current) {
            streamRef.current.getTracks().forEach((track) => track.stop());
            streamRef.current = null;
        }

        if (videoRef.current) {
            videoRef.current.srcObject = null;
        }

        setIsScanning(false);
    };

    // ðŸ”„ Scan again - restart everything
    const scanAgain = () => {
        setValidHit(null);
        setHitHistory([]);
        startScanning();
    };

    // ðŸ”¢ Compute cosine similarity between two vectors
    const cosineSimilarity = (a: number[], b: number[]): number => {
        if (a.length !== b.length) {
            throw new Error("Vectors must have same length");
        }

        let dotProduct = 0;
        let normA = 0;
        let normB = 0;

        for (let i = 0; i < a.length; i++) {
            dotProduct += a[i] * b[i];
            normA += a[i] * a[i];
            normB += b[i] * b[i];
        }

        normA = Math.sqrt(normA);
        normB = Math.sqrt(normB);

        if (normA === 0 || normB === 0) {
            return 0;
        }

        return dotProduct / (normA * normB);
    };

    // ðŸ” Find top N nearest neighbors in embeddings database
    const findTopNCards = (queryEmbedding: number[], topN: number = 3): Array<{ cardId: string; similarity: number }> => {
        if (!embeddingsDatabaseRef.current) {
            throw new Error("Embeddings database not loaded");
        }

        const allMatches: Array<{ cardId: string; similarity: number }> = [];

        for (const entry of embeddingsDatabaseRef.current.embeddings) {
            const similarity = cosineSimilarity(queryEmbedding, entry.embedding);
            allMatches.push({
                cardId: entry.card_id,
                similarity: similarity,
            });
        }

        // Sort by similarity (highest first) and return top N
        return allMatches.sort((a, b) => b.similarity - a.similarity).slice(0, topN);
    };

    // ðŸŽ¯ Capture frame and run embedding extraction + similarity search
    const captureAndPredict = async () => {
        if (!videoRef.current || !canvasRef.current || !modelRef.current) return;

        const video = videoRef.current;
        const canvas = canvasRef.current;
        const context = canvas.getContext("2d");

        if (!context || video.readyState !== video.HAVE_ENOUGH_DATA) return;

        try {
            const videoWidth = video.videoWidth;
            const videoHeight = video.videoHeight;

            if (!videoWidth || !videoHeight) {
                // console.warn("âš ï¸ videoWidth/videoHeight is zero â€” metadata not ready");
                return;
            }

            // Crop to card aspect ratio first (~0.72:1 like training data 320Ã—440)
            const CARD_ASPECT = 320 / 440; // ~0.727
            const videoAspect = videoWidth / videoHeight;

            let sx = 0;
            let sy = 0;
            let sWidth = videoWidth;
            let sHeight = videoHeight;

            if (videoAspect > CARD_ASPECT) {
                // Video is wider â†’ crop left/right
                sHeight = videoHeight;
                sWidth = sHeight * CARD_ASPECT;
                sx = (videoWidth - sWidth) / 2;
                sy = 0;
            } else {
                // Video is taller â†’ crop top/bottom
                sWidth = videoWidth;
                sHeight = sWidth / CARD_ASPECT;
                sx = 0;
                sy = (videoHeight - sHeight) / 2;
            }

            // Prepare image for MobileNet embedding extraction (224Ã—224)
            const TARGET_SIZE = 224;
            canvas.width = TARGET_SIZE;
            canvas.height = TARGET_SIZE;

            // Draw cropped card region and resize to 224Ã—224 for embeddings
            context.drawImage(video, sx, sy, sWidth, sHeight, 0, 0, TARGET_SIZE, TARGET_SIZE);

            // Convert to tensor and extract embeddings with MobileNet
            const imageData = context.getImageData(0, 0, TARGET_SIZE, TARGET_SIZE);

            const embeddingTensor = tf.tidy(() => {
                // Convert canvas to tensor [224, 224, 3]
                const imageTensor = tf.browser.fromPixels(imageData);

                // Extract embedding using MobileNet's infer method (1024-dim)
                // The second parameter (true) returns embeddings instead of classifications
                const embeddingRaw = modelRef.current!.infer(imageTensor as unknown as tf.Tensor3D, true) as unknown as tf.Tensor;

                // Squeeze to remove batch dimension if present: [1, 1024] -> [1024]
                const embedding = embeddingRaw.squeeze() as unknown as tf.Tensor;

                // L2 normalize (same as training)
                const norm = tf.norm(embedding, 2, -1, true);
                const normalizedEmbedding = embedding.div(norm).squeeze() as tf.Tensor;

                // Return 1D tensor [1024]
                return normalizedEmbedding;
            });

            // Get embedding as array
            const embeddingArray = await embeddingTensor.data();
            const embeddingVector = Array.from(embeddingArray);

            // Find top 3 nearest cards
            const topMatches = findTopNCards(embeddingVector, 3);

            // console.log("ðŸ” Top 3 Similarity Results:", topMatches.map((m, i) => ({
            //     rank: i + 1,
            //     cardId: m.cardId,
            //     similarity: (m.similarity * 100).toFixed(1) + "%",
            // })));

            // Cleanup
            embeddingTensor.dispose();

            // ðŸ”¤ OCR: Only run if all top 3 matches are >65% similarity
            const allAboveThreshold = topMatches.every((m) => m.similarity * 100 > 65);

            if (allAboveThreshold) {
                // Create a temporary canvas for OCR with original resolution
                const ocrCanvas = document.createElement("canvas");
                const ocrContext = ocrCanvas.getContext("2d");

                if (ocrContext) {
                    // Only use top half of the card (where the title is)
                    const ocrHeight = sHeight / 2;

                    // Set canvas to top half only
                    ocrCanvas.width = sWidth;
                    ocrCanvas.height = ocrHeight;

                    // Draw only the top half of the cropped card region
                    ocrContext.drawImage(
                        video,
                        sx,
                        sy, // Source x, y (top-left of card)
                        sWidth,
                        ocrHeight, // Source width, height (top half only)
                        0,
                        0, // Dest x, y
                        sWidth,
                        ocrHeight // Dest width, height
                    );

                    // Preprocessing: Enhance image for better OCR
                    const imageData = ocrContext.getImageData(0, 0, sWidth, ocrHeight);
                    const data = imageData.data;

                    // Apply brightness, contrast, and grayscale (more aggressive)
                    const brightness = 50; // Higher brightness
                    const contrast = 60; // Higher contrast

                    for (let i = 0; i < data.length; i += 4) {
                        // Convert to grayscale first (helps OCR focus on text)
                        const gray = 0.299 * data[i] + 0.587 * data[i + 1] + 0.114 * data[i + 2];

                        // Apply brightness and contrast
                        let value = gray;
                        value += brightness;
                        value = ((value - 128) * (contrast + 100)) / 100 + 128;

                        // Clamp to valid range
                        value = Math.max(0, Math.min(255, value));

                        // Apply to all RGB channels
                        data[i] = value; // R
                        data[i + 1] = value; // G
                        data[i + 2] = value; // B
                        // Alpha channel (data[i + 3]) stays unchanged
                    }

                    // Put the processed image back
                    ocrContext.putImageData(imageData, 0, 0);

                    try {
                        // Configure Tesseract for single line mode (better for Pokemon names)
                        const ocrResult = await Tesseract.recognize(ocrCanvas, "eng", {
                            tesseract_parameters: {
                                tessedit_pageseg_mode: "7", // Single line of text
                                tessedit_char_whitelist: "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz ",
                            },
                        } as Record<string, unknown>);

                        console.log("ðŸ“ OCR Text Detected:", ocrResult.data.text);
                        console.log("ðŸ“Š OCR Confidence:", ocrResult.data.confidence.toFixed(1) + "%");

                        // Use fuzzy matching to check if any top 3 cards appear in OCR text
                        if (!pokemonNamesFuseRef.current) {
                            console.error("âŒ Pokemon names database not loaded");
                            return;
                        }

                        const ocrText = ocrResult.data.text.toLowerCase().trim();

                        // Split OCR text into words and clean them
                        const ocrWords = ocrText
                            .split(/\s+/)
                            .map((word) => word.replace(/[^a-z0-9]/gi, "").toLowerCase())
                            .filter((word) => word.length > 2); // Filter out very short words

                        console.log("ðŸ” OCR words:", ocrWords.join(", "));

                        // Check each of the top 3 embedding matches
                        for (const match of topMatches) {
                            // Extract card name from card ID (e.g., "sv02-085_Slowpoke" -> "Slowpoke")
                            const cardName = match.cardId.split("_").slice(1).join("_");
                            const cardNameClean = cardName.replace(/_/g, " ").toLowerCase();

                            // Try exact substring match first
                            if (ocrText.includes(cardNameClean)) {
                                console.log("âœ… VALID HIT! (Exact match) Card:", cardName, "| Similarity:", (match.similarity * 100).toFixed(1) + "%");
                                const validResult: ScanResult = {
                                    cardId: match.cardId,
                                    cardName: cardName,
                                    similarity: match.similarity * 100,
                                };
                                setValidHit(validResult);
                                stopScanning();
                                return;
                            }

                            // Try fuzzy matching each OCR word against the card name
                            for (const word of ocrWords) {
                                const fuseResults = pokemonNamesFuseRef.current!.search(word);
                                if (fuseResults.length > 0) {
                                    const bestMatch = fuseResults[0];
                                    const matchedName = bestMatch.item.toLowerCase();
                                    const matchScore = bestMatch.score || 0;

                                    // Check if fuzzy match is the card we're checking
                                    if (matchedName === cardNameClean) {
                                        console.log("âœ… VALID HIT! (Fuzzy match) OCR word:", word, "â†’", matchedName, "| Score:", (matchScore * 100).toFixed(1) + "%");
                                        const validResult: ScanResult = {
                                            cardId: match.cardId,
                                            cardName: cardName,
                                            similarity: match.similarity * 100,
                                        };
                                        setValidHit(validResult);
                                        stopScanning();
                                        return;
                                    }
                                }
                            }
                        }

                        // If we get here, none of the top 3 matched
                        console.log("âŒ No match - Top 3:", topMatches.map((m) => m.cardId.split("_").slice(1).join("_")).join(", "), "| OCR words:", ocrWords.join(", "));
                    } catch (ocrError) {
                        console.error("âŒ OCR Error:", ocrError);
                    }
                }
            } else {
                console.log("â­ï¸ Skipping OCR - not all top 3 above 65%:", topMatches.map((m) => (m.similarity * 100).toFixed(1) + "%").join(", "));
            }
        } catch (err) {
            console.error("âŒ Prediction error:", err);
        }
    };

    // ðŸ§¹ Cleanup on unmount
    useEffect(() => {
        return () => {
            stopScanning();
            // Note: MobileNet doesn't have a dispose method
        };
    }, []);

    return (
        <div className="app">
            <header>
                <h1>CGP Card Scanner</h1>
                <p>AI-powered instant card recognition (MobileNet v2 embeddings)</p>
                <div className="model-status">{modelStatus}</div>
            </header>

            <main>
                {/* Error display */}
                {error && (
                    <div className="error">
                        <strong>Error:</strong> {error}
                    </div>
                )}

                {/* Valid hit result - only shown after 3 consecutive matches */}
                {validHit && (
                    <div className="result result--confirmed">
                        <div className="result-header">âœ… Card Identified!</div>
                        <div className="result-content">
                            <div className="card-id">{validHit.cardId}</div>
                            <div className="card-name">{validHit.cardName}</div>
                            <div className="card-meta card-meta--high-confidence">
                                <span>Confidence: {validHit.similarity.toFixed(1)}%</span>
                            </div>
                        </div>
                    </div>
                )}

                {/* Scanner container - only show when actively scanning */}
                <div className="scanner-container" style={{ display: isScanning ? "flex" : "none" }}>
                    {/* Video stream */}
                    <div className="video-wrapper">
                        <video ref={videoRef} autoPlay playsInline muted className="active" />
                    </div>

                    {/* Hidden canvas for frame capture */}
                    <canvas ref={canvasRef} style={{ display: "none" }} />
                </div>

                {/* Control buttons */}
                {!validHit && !isScanning && (
                    <button onClick={startScanning} className="btn-primary" disabled={!isModelLoaded}>
                        Start Scanner
                    </button>
                )}

                {!validHit && isScanning && (
                    <button onClick={stopScanning} className="btn-secondary">
                        Stop Scanner
                    </button>
                )}

                {validHit && (
                    <button onClick={scanAgain} className="btn-primary">
                        Scan Again
                    </button>
                )}

                {/* Scanning indicator - only shown while actively scanning */}
                {isScanning && !validHit && (
                    <div className="scanning-indicator">
                        <div className="spinner"></div>
                        <p>Scanning for cards...</p>
                        <div style={{ fontSize: "0.9rem", color: "#808080", marginTop: "0.5rem" }}>
                            {hitHistory.length > 0 && (
                                <>
                                    Hits: {hitHistory.slice(-3).length}/3
                                    {hitHistory.length >= 3 && <span style={{ marginLeft: "1rem" }}>{hitHistory.slice(-3).every((h) => h.cardId === hitHistory[hitHistory.length - 1].cardId) ? "ðŸŽ¯ Same card detected" : "ðŸ”„ Different cards"}</span>}
                                </>
                            )}
                        </div>
                    </div>
                )}
            </main>
        </div>
    );
}

export default App;
