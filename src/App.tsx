import { useState, useRef, useEffect } from "react";
import * as tf from "@tensorflow/tfjs";
import * as mobilenet from "@tensorflow-models/mobilenet";
import "./App.css";

interface ScanResult {
    cardId: string;
    cardName: string;
    similarity: number; // cosine similarity in [0, 100]
}

interface Hit {
    cardId: string;
    similarity: number;
}

interface EmbeddingData {
    card_id: string;
    filename: string;
    embedding: number[];
}

interface EmbeddingsDatabase {
    model: string;
    embedding_dim: number;
    total_images: number;
    total_cards: number;
    embeddings: EmbeddingData[];
}

function App() {
    const [isScanning, setIsScanning] = useState(false);
    const [validHit, setValidHit] = useState<ScanResult | null>(null);
    const [hitHistory, setHitHistory] = useState<Hit[]>([]);
    const [error, setError] = useState<string | null>(null);
    const [modelStatus, setModelStatus] = useState<string>("Loading model...");
    const [isModelLoaded, setIsModelLoaded] = useState(false);

    const videoRef = useRef<HTMLVideoElement>(null);
    const canvasRef = useRef<HTMLCanvasElement>(null);
    const streamRef = useRef<MediaStream | null>(null);
    const intervalRef = useRef<number | null>(null);

    // MobileNet model reference
    const modelRef = useRef<mobilenet.MobileNet | null>(null);
    const embeddingsDatabaseRef = useRef<EmbeddingsDatabase | null>(null);

    // ðŸ§  Load MobileNet feature extractor + embeddings database
    useEffect(() => {
        async function loadModel() {
            try {
                setModelStatus("Loading embeddings database...");

                // Load embeddings database (generated by TypeScript script)
                const embeddingsResponse = await fetch("/embeddings/embeddings.json");
                const embeddingsData: EmbeddingsDatabase = await embeddingsResponse.json();
                embeddingsDatabaseRef.current = embeddingsData;

                console.log(`âœ… Loaded ${embeddingsData.total_images} embeddings for ${embeddingsData.total_cards} cards`);
                console.log(`   Model: ${embeddingsData.model}`);
                console.log(`   Embedding dimension: ${embeddingsData.embedding_dim}`);

                setModelStatus("Loading MobileNet model...");

                // Load MobileNet v2 from TensorFlow.js (same as used for embeddings generation)
                const featureExtractor = await mobilenet.load({
                    version: 2,
                    alpha: 1.0, // Full model (not quantized)
                });
                modelRef.current = featureExtractor;

                console.log("âœ… MobileNet v2 loaded successfully");

                setModelStatus(`Model loaded! ${embeddingsData.total_cards} cards ready (${embeddingsData.total_images} variations)`);
                setIsModelLoaded(true);
            } catch (err) {
                console.error("Failed to load model:", err);
                setError("Failed to load model: " + (err as Error).message);
                setModelStatus("Model load failed");
                setIsModelLoaded(false);
            }
        }

        loadModel();
    }, []);

    // ðŸ“¸ Start camera and scanning
    const startScanning = async () => {
        if (!modelRef.current) {
            setError("Model not loaded yet. Please wait...");
            return;
        }

        try {
            setError(null);
            setValidHit(null); // Clear previous valid hit
            setHitHistory([]); // Clear hit history

            // Request camera access
            const stream = await navigator.mediaDevices.getUserMedia({
                video: {
                    facingMode: "environment",
                    width: { ideal: 720 },
                    height: { ideal: 1000 },
                    aspectRatio: { ideal: 0.72 },
                },
            });

            if (videoRef.current) {
                videoRef.current.srcObject = stream;
                streamRef.current = stream;
                setIsScanning(true);

                // Start capturing frames every 250ms
                intervalRef.current = window.setInterval(() => {
                    captureAndPredict();
                }, 250);
            }
        } catch (err) {
            setError("Failed to access camera: " + (err as Error).message);
        }
    };

    // ðŸ›‘ Stop camera and scanning
    const stopScanning = () => {
        if (intervalRef.current) {
            clearInterval(intervalRef.current);
            intervalRef.current = null;
        }

        if (streamRef.current) {
            streamRef.current.getTracks().forEach((track) => track.stop());
            streamRef.current = null;
        }

        if (videoRef.current) {
            videoRef.current.srcObject = null;
        }

        setIsScanning(false);
    };

    // ðŸ”„ Scan again - restart everything
    const scanAgain = () => {
        setValidHit(null);
        setHitHistory([]);
        startScanning();
    };

    // ðŸ”¢ Compute cosine similarity between two vectors
    const cosineSimilarity = (a: number[], b: number[]): number => {
        if (a.length !== b.length) {
            throw new Error("Vectors must have same length");
        }

        let dotProduct = 0;
        let normA = 0;
        let normB = 0;

        for (let i = 0; i < a.length; i++) {
            dotProduct += a[i] * b[i];
            normA += a[i] * a[i];
            normB += b[i] * b[i];
        }

        normA = Math.sqrt(normA);
        normB = Math.sqrt(normB);

        if (normA === 0 || normB === 0) {
            return 0;
        }

        return dotProduct / (normA * normB);
    };

    // ðŸ” Find top N nearest neighbors in embeddings database
    const findTopNCards = (queryEmbedding: number[], topN: number = 3): Array<{ cardId: string; similarity: number }> => {
        if (!embeddingsDatabaseRef.current) {
            throw new Error("Embeddings database not loaded");
        }

        const allMatches: Array<{ cardId: string; similarity: number }> = [];

        for (const entry of embeddingsDatabaseRef.current.embeddings) {
            const similarity = cosineSimilarity(queryEmbedding, entry.embedding);
            allMatches.push({
                cardId: entry.card_id,
                similarity: similarity,
            });
        }

        // Sort by similarity (highest first) and return top N
        return allMatches.sort((a, b) => b.similarity - a.similarity).slice(0, topN);
    };

    // âœ… Check if we have a valid hit (3 consecutive hits on same card with >65% similarity)
    const checkForValidHit = (newHitHistory: Hit[]) => {
        if (newHitHistory.length < 3) {
            return null;
        }

        // Get last 3 hits
        const lastThree = newHitHistory.slice(-3);

        // Check if all 3 are the same card
        const firstCardId = lastThree[0].cardId;
        const allSameCard = lastThree.every((hit) => hit.cardId === firstCardId);

        if (!allSameCard) {
            return null;
        }

        // Check if all have >65% similarity
        const allAboveThreshold = lastThree.every((hit) => hit.similarity > 65);

        if (!allAboveThreshold) {
            return null;
        }

        // Valid hit! Return the result
        const cardName = firstCardId.split("_").slice(1).join(" ");
        const avgSimilarity = lastThree.reduce((sum, hit) => sum + hit.similarity, 0) / 3;

        return {
            cardId: firstCardId,
            cardName,
            similarity: avgSimilarity,
        };
    };

    // ðŸŽ¯ Capture frame and run embedding extraction + similarity search
    const captureAndPredict = async () => {
        if (!videoRef.current || !canvasRef.current || !modelRef.current) return;

        const video = videoRef.current;
        const canvas = canvasRef.current;
        const context = canvas.getContext("2d");

        if (!context || video.readyState !== video.HAVE_ENOUGH_DATA) return;

        try {
            // MobileNet expects 224Ã—224 input
            const TARGET_SIZE = 224;

            // Set canvas to model input size
            canvas.width = TARGET_SIZE;
            canvas.height = TARGET_SIZE;

            const videoWidth = video.videoWidth;
            const videoHeight = video.videoHeight;

            if (!videoWidth || !videoHeight) {
                console.warn("âš ï¸ videoWidth/videoHeight is zero â€” metadata not ready");
                return;
            }

            // Crop to card aspect ratio first (~0.72:1 like training data 320Ã—440)
            const CARD_ASPECT = 320 / 440; // ~0.727
            const videoAspect = videoWidth / videoHeight;

            let sx = 0;
            let sy = 0;
            let sWidth = videoWidth;
            let sHeight = videoHeight;

            if (videoAspect > CARD_ASPECT) {
                // Video is wider â†’ crop left/right
                sHeight = videoHeight;
                sWidth = sHeight * CARD_ASPECT;
                sx = (videoWidth - sWidth) / 2;
                sy = 0;
            } else {
                // Video is taller â†’ crop top/bottom
                sWidth = videoWidth;
                sHeight = sWidth / CARD_ASPECT;
                sx = 0;
                sy = (videoHeight - sHeight) / 2;
            }

            // Draw cropped card region and resize to 224Ã—224
            context.drawImage(video, sx, sy, sWidth, sHeight, 0, 0, TARGET_SIZE, TARGET_SIZE);

            // Convert to tensor and extract embeddings with MobileNet
            const imageData = context.getImageData(0, 0, TARGET_SIZE, TARGET_SIZE);

            const embeddingTensor = tf.tidy(() => {
                // Convert canvas to tensor [224, 224, 3]
                const imageTensor = tf.browser.fromPixels(imageData);

                // Extract embedding using MobileNet's infer method (1024-dim)
                // The second parameter (true) returns embeddings instead of classifications
                const embeddingRaw = modelRef.current!.infer(imageTensor as unknown as tf.Tensor3D, true) as unknown as tf.Tensor;

                // Squeeze to remove batch dimension if present: [1, 1024] -> [1024]
                const embedding = embeddingRaw.squeeze() as unknown as tf.Tensor;

                // L2 normalize (same as training)
                const norm = tf.norm(embedding, 2, -1, true);
                const normalizedEmbedding = embedding.div(norm).squeeze() as tf.Tensor;

                // Return 1D tensor [1024]
                return normalizedEmbedding;
            });

            // Get embedding as array
            const embeddingArray = await embeddingTensor.data();
            const embeddingVector = Array.from(embeddingArray);

            // Find top 3 nearest cards
            const topMatches = findTopNCards(embeddingVector, 3);
            const match = topMatches[0]; // Use best match for logic

            console.log("ðŸ” Top 3 Similarity Results:", topMatches.map((m, i) => ({
                rank: i + 1,
                cardId: m.cardId,
                similarity: (m.similarity * 100).toFixed(1) + "%",
            })));

            // Cleanup
            embeddingTensor.dispose();

            // Add to hit history
            const newHit: Hit = {
                cardId: match.cardId,
                similarity: match.similarity * 100,
            };

            setHitHistory((prev) => {
                const updated = [...prev, newHit];

                // Check for valid hit
                const validResult = checkForValidHit(updated);
                if (validResult) {
                    console.log("âœ… VALID HIT DETECTED:", validResult);
                    setValidHit(validResult);
                    stopScanning(); // Stop scanning when valid hit is detected
                }

                // Keep only last 10 hits to avoid memory issues
                return updated.slice(-10);
            });
        } catch (err) {
            console.error("âŒ Prediction error:", err);
        }
    };

    // ðŸ§¹ Cleanup on unmount
    useEffect(() => {
        return () => {
            stopScanning();
            // Note: MobileNet doesn't have a dispose method
        };
    }, []);

    return (
        <div className="app">
            <header>
                <h1>CGP Card Scanner</h1>
                <p>AI-powered instant card recognition (MobileNet v2 embeddings)</p>
                <div className="model-status">{modelStatus}</div>
            </header>

            <main>
                {/* Error display */}
                {error && (
                    <div className="error">
                        <strong>Error:</strong> {error}
                    </div>
                )}

                {/* Valid hit result - only shown after 3 consecutive matches */}
                {validHit && (
                    <div className="result result--confirmed">
                        <div className="result-header">âœ… Card Identified!</div>
                        <div className="result-content">
                            <div className="card-id">{validHit.cardId}</div>
                            <div className="card-name">{validHit.cardName}</div>
                            <div className="card-meta card-meta--high-confidence">
                                <span>Confidence: {validHit.similarity.toFixed(1)}%</span>
                            </div>
                        </div>
                    </div>
                )}

                {/* Scanner container - only show when actively scanning */}
                <div className="scanner-container" style={{ display: isScanning ? "flex" : "none" }}>
                    {/* Video stream */}
                    <div className="video-wrapper">
                        <video ref={videoRef} autoPlay playsInline muted className="active" />
                    </div>

                    {/* Hidden canvas for frame capture */}
                    <canvas ref={canvasRef} style={{ display: "none" }} />
                </div>

                {/* Control buttons */}
                {!validHit && !isScanning && (
                    <button onClick={startScanning} className="btn-primary" disabled={!isModelLoaded}>
                        Start Scanner
                    </button>
                )}

                {!validHit && isScanning && (
                    <button onClick={stopScanning} className="btn-secondary">
                        Stop Scanner
                    </button>
                )}

                {validHit && (
                    <button onClick={scanAgain} className="btn-primary">
                        Scan Again
                    </button>
                )}

                {/* Scanning indicator - only shown while actively scanning */}
                {isScanning && !validHit && (
                    <div className="scanning-indicator">
                        <div className="spinner"></div>
                        <p>Scanning for cards...</p>
                        <div style={{ fontSize: "0.9rem", color: "#808080", marginTop: "0.5rem" }}>
                            {hitHistory.length > 0 && (
                                <>
                                    Hits: {hitHistory.slice(-3).length}/3
                                    {hitHistory.length >= 3 && <span style={{ marginLeft: "1rem" }}>{hitHistory.slice(-3).every((h) => h.cardId === hitHistory[hitHistory.length - 1].cardId) ? "ðŸŽ¯ Same card detected" : "ðŸ”„ Different cards"}</span>}
                                </>
                            )}
                        </div>
                    </div>
                )}
            </main>
        </div>
    );
}

export default App;
